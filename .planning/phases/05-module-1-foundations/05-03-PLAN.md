---
phase: 05-module-1-foundations
plan: 03
type: execute
wave: 2
depends_on: ["05-02"]
files_modified:
  - src/content/course/module-1/05-python-consumer.mdx
  - src/content/course/module-1/06-event-structure.mdx
autonomous: true

must_haves:
  truths:
    - "Students can write Python code to consume CDC events from Kafka using confluent-kafka"
    - "Students can parse CDC event structure (envelope, before/after, metadata fields)"
  artifacts:
    - path: "src/content/course/module-1/05-python-consumer.mdx"
      provides: "Python consumer lesson with confluent-kafka"
      contains: "confluent_kafka"
      min_lines: 200
    - path: "src/content/course/module-1/06-event-structure.mdx"
      provides: "CDC event structure parsing lesson"
      contains: "def parse_cdc_event"
      min_lines: 220
  key_links:
    - from: "src/content/course/module-1/05-python-consumer.mdx"
      to: "confluent-kafka library"
      via: "from confluent_kafka import Consumer"
      pattern: "from confluent_kafka import"
    - from: "src/content/course/module-1/06-event-structure.mdx"
      to: "parse_cdc_event function"
      via: "complete function with all operation types"
      pattern: "def parse_cdc_event.*[\\s\\S]*'r':.*'c':.*'u':.*'d':"
---

<objective>
Create the Python coding lessons: Consuming CDC events and parsing event structure.

Purpose: Students transition from using CLI tools (kafka-console-consumer) to writing production-quality Python code. These lessons teach practical skills they'll use in real data engineering work.

Output: Two MDX lesson files with complete, runnable Python code using confluent-kafka library.
</objective>

<execution_context>
@/Users/levoely/.claude/get-shit-done/workflows/execute-plan.md
@/Users/levoely/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/05-module-1-foundations/05-RESEARCH.md

# Lab environment with JupyterLab
@labs/docker-compose.yml
@labs/jupyter/requirements.txt

# Existing content patterns
@src/content/course/01-intro/index.mdx
@src/content/config.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Python Consumer Lesson</name>
  <files>src/content/course/module-1/05-python-consumer.mdx</files>
  <action>
Create the Python consumer lesson with these sections:

**Frontmatter:**
```yaml
title: "Потребление CDC событий на Python"
description: "Написание Python consumer с использованием confluent-kafka для обработки CDC событий"
order: 5
difficulty: "intermediate"
estimatedTime: 30
topics: ["Python", "confluent-kafka", "Consumer", "Event Processing"]
prerequisites: ["module-1/04-first-connector"]
```

**Content structure:**

1. **Why confluent-kafka (not kafka-python):**
   - Built on librdkafka (C library) - 10x faster
   - Official Confluent support
   - Schema Registry integration for later modules
   - ARM64 native wheels (confluent-kafka 2.13.0+)

2. **IMPORTANT - Hostname warning box (prominent callout):**
   Create a warning/info box explaining:
   ```
   Когда использовать kafka:9092 vs localhost:9092?

   - kafka:9092 - используйте внутри Docker сети (JupyterLab, другие контейнеры)
   - localhost:9092 - используйте с вашего хоста (терминал Mac/Windows)

   В этом уроке мы работаем в JupyterLab (внутри Docker), поэтому используем kafka:9092.
   Если бы вы запускали Python скрипт на своем компьютере (не в контейнере),
   нужен был бы localhost:9092.
   ```

3. **Access JupyterLab:**
   - Open http://localhost:8888
   - confluent-kafka already installed in container
   - Create new notebook or use existing template

4. **Basic consumer setup:**
   ```python
   from confluent_kafka import Consumer
   import json

   config = {
       'bootstrap.servers': 'kafka:9092',  # Internal Docker hostname - see note above!
       'group.id': 'cdc-tutorial-group',
       'auto.offset.reset': 'earliest',
       'enable.auto.commit': True
   }

   consumer = Consumer(config)
   consumer.subscribe(['inventory.public.customers'])
   ```

5. **Explain configuration options:**
   - `bootstrap.servers`: Kafka broker address (use `kafka` inside Docker network, `localhost` from host)
   - `group.id`: Consumer group for offset tracking
   - `auto.offset.reset`: Where to start if no offset exists
   - `enable.auto.commit`: Automatic offset commits

6. **Basic polling loop:**
   ```python
   print("Waiting for CDC events... (Ctrl+C to stop)")

   try:
       while True:
           msg = consumer.poll(timeout=1.0)

           if msg is None:
               continue

           if msg.error():
               print(f"Error: {msg.error()}")
               continue

           # Parse and print event
           event = json.loads(msg.value().decode('utf-8'))
           print(f"Received: {event}")

   except KeyboardInterrupt:
       print("Stopping...")
   finally:
       consumer.close()
   ```

7. **Complete working example:**
   Combine all pieces into one copy-pasteable cell:
   ```python
   from confluent_kafka import Consumer
   import json

   # NOTE: Use 'kafka:9092' in JupyterLab (inside Docker)
   #       Use 'localhost:9092' if running from your host machine
   config = {
       'bootstrap.servers': 'kafka:9092',
       'group.id': 'cdc-tutorial-group',
       'auto.offset.reset': 'earliest',
       'enable.auto.commit': True
   }

   consumer = Consumer(config)
   consumer.subscribe(['inventory.public.customers'])

   print("Listening for CDC events on inventory.public.customers...")
   print("Insert/update/delete rows in PostgreSQL to see events")
   print("Press Ctrl+C (or interrupt kernel) to stop\n")

   try:
       while True:
           msg = consumer.poll(timeout=1.0)

           if msg is None:
               continue

           if msg.error():
               print(f"Consumer error: {msg.error()}")
               continue

           try:
               event = json.loads(msg.value().decode('utf-8'))
               payload = event.get('payload', {})

               op = payload.get('op', '?')
               op_name = {'c': 'CREATE', 'u': 'UPDATE', 'd': 'DELETE', 'r': 'SNAPSHOT'}.get(op, op)

               print(f"[{op_name}] {payload.get('after') or payload.get('before')}")

           except (json.JSONDecodeError, UnicodeDecodeError) as e:
               print(f"Decode error: {e}")
               continue

   except KeyboardInterrupt:
       print("\nStopping consumer...")
   finally:
       consumer.close()
       print("Consumer closed.")
   ```

8. **Testing the consumer:**
   - Run the cell in JupyterLab
   - In another terminal: insert/update/delete rows
   - Watch events appear in real-time

9. **poll() timeout explained:**
   - `timeout=1.0`: Wait up to 1 second for messages
   - Returns `None` if no message
   - Lower values = more responsive, higher CPU
   - Recommended: 1.0 for demos, 0.1 for production

**Important notes:**
- **Prominent warning about kafka:9092 vs localhost:9092** - this is a common source of confusion
- JupyterLab runs inside Docker network, hence kafka:9092
- Code must be complete and runnable (no "..." placeholders)
  </action>
  <verify>
1. File exists at src/content/course/module-1/05-python-consumer.mdx
2. Uses `from confluent_kafka import Consumer` (not kafka-python)
3. Contains complete, runnable code example
4. Uses `kafka:9092` for bootstrap servers (Docker internal)
5. **Has prominent warning/callout explaining when to use localhost vs kafka hostname**
6. Has error handling for JSONDecodeError
7. Explains poll() timeout semantics
8. Run `npm run build` - no errors
  </verify>
  <done>
Lesson file exists with complete Python consumer code using confluent-kafka. Includes prominent warning explaining kafka:9092 (inside Docker/JupyterLab) vs localhost:9092 (from host machine). Error handling and clear explanations for running in JupyterLab.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create Event Structure Lesson</name>
  <files>src/content/course/module-1/06-event-structure.mdx</files>
  <action>
Create the event structure parsing lesson with these sections:

**Frontmatter:**
```yaml
title: "Структура CDC событий Debezium"
description: "Разбор envelope-формата, поля before/after, metadata источника и типы операций"
order: 6
difficulty: "intermediate"
estimatedTime: 25
topics: ["Debezium", "Event Structure", "JSON", "Python"]
prerequisites: ["module-1/05-python-consumer"]
```

**Content structure:**

1. **Debezium envelope structure:**
   Show complete event with annotations:
   ```json
   {
     "schema": { ... },  // Schema definition (can be ignored for now)
     "payload": {
       "before": null,   // State BEFORE change (null for INSERT)
       "after": {        // State AFTER change (null for DELETE)
         "id": 3,
         "name": "Алексей Козлов",
         "email": "[email protected]",
         "created_at": 1706745600000000
       },
       "source": {       // Metadata about the source
         "version": "2.5.4.Final",
         "connector": "postgresql",
         "name": "inventory",
         "ts_ms": 1706745600123,
         "snapshot": "false",
         "db": "inventory",
         "schema": "public",
         "table": "customers",
         "txId": 1234,
         "lsn": 23456789
       },
       "op": "c",        // Operation: c=create, u=update, d=delete, r=read
       "ts_ms": 1706745600456,
       "transaction": null
     }
   }
   ```

2. **Operation types explained:**
   | op | Name | before | after | When |
   |----|------|--------|-------|------|
   | r | read (snapshot) | null | data | Initial snapshot |
   | c | create | null | data | INSERT |
   | u | update | old_data | new_data | UPDATE |
   | d | delete | old_data | null | DELETE |

3. **Common pitfall: Null handling:**
   ```python
   # WRONG - crashes on DELETE
   customer_id = event['payload']['after']['id']

   # RIGHT - handle null cases
   payload = event['payload']
   op = payload['op']

   if op == 'c' or op == 'r':
       data = payload['after']  # Safe - after exists
   elif op == 'u':
       old_data = payload['before']
       new_data = payload['after']
   elif op == 'd':
       data = payload['before']  # Safe - before exists, after is null
   ```

4. **Source metadata fields:**
   - `ts_ms`: When change was made in database (milliseconds)
   - `snapshot`: "true" if from initial snapshot, "false" if live
   - `db`, `schema`, `table`: Source location
   - `txId`: Transaction ID in PostgreSQL
   - `lsn`: Log Sequence Number (position in WAL)

5. **Mermaid diagram: Event anatomy**

6. **Complete reusable parsing function (CRITICAL - must handle ALL operation types):**
   ```python
   def parse_cdc_event(raw_event):
       """
       Parse Debezium CDC event into structured format.

       Handles all operation types:
       - 'r' (read/snapshot): Initial data load
       - 'c' (create): INSERT
       - 'u' (update): UPDATE
       - 'd' (delete): DELETE

       Args:
           raw_event: dict from json.loads(message.value())

       Returns:
           dict with operation, before, after, and source metadata
       """
       payload = raw_event.get('payload', {})
       source = payload.get('source', {})

       op = payload.get('op')
       op_names = {
           'r': 'snapshot',  # Read - initial snapshot
           'c': 'create',    # Create - INSERT
           'u': 'update',    # Update - UPDATE
           'd': 'delete'     # Delete - DELETE
       }

       return {
           'operation': op_names.get(op, f'unknown({op})'),
           'operation_code': op,  # Original code for switching
           'before': payload.get('before'),  # null for r, c
           'after': payload.get('after'),    # null for d
           'is_snapshot': source.get('snapshot') == 'true',
           'table': f"{source.get('schema')}.{source.get('table')}",
           'database': source.get('db'),
           'timestamp_ms': payload.get('ts_ms'),
           'transaction_id': source.get('txId'),
           'lsn': source.get('lsn')
       }
   ```

7. **Using the parser with all operation types:**
   ```python
   from confluent_kafka import Consumer
   import json

   # ... consumer setup ...

   while True:
       msg = consumer.poll(timeout=1.0)
       if msg is None or msg.error():
           continue

       raw = json.loads(msg.value().decode('utf-8'))
       event = parse_cdc_event(raw)

       print(f"[{event['operation'].upper()}] {event['table']}")

       # Handle each operation type explicitly
       op = event['operation_code']

       if op == 'r':  # Snapshot (initial load)
           print(f"  Snapshot data: {event['after']}")
       elif op == 'c':  # Create (INSERT)
           print(f"  New row: {event['after']}")
       elif op == 'u':  # Update
           print(f"  Changed from: {event['before']}")
           print(f"  Changed to:   {event['after']}")
       elif op == 'd':  # Delete
           print(f"  Deleted: {event['before']}")
       else:
           print(f"  Unknown operation: {op}")
   ```

8. **Snapshot vs streaming distinction:**
   - First events after connector start are snapshot (op: "r")
   - Subsequent events are live changes (op: "c", "u", "d")
   - Important for deduplication logic in data pipelines

9. **Timestamp handling:**
   ```python
   from datetime import datetime

   # Debezium timestamps are in milliseconds
   ts_ms = event['timestamp_ms']
   dt = datetime.fromtimestamp(ts_ms / 1000)
   print(f"Event time: {dt.isoformat()}")
   ```

10. **Module 1 summary:**
    - We learned: CDC concept, Debezium architecture, lab setup, connector deployment, Python consumer, event parsing
    - Next: Module 2 covers PostgreSQL deep dive (replication slots, WAL configuration, Aurora specifics)

**Important notes:**
- Show complete JSON structures, not truncated
- Emphasize null handling (most common beginner mistake)
- Include timestamp conversion for practical use
- parse_cdc_event MUST explicitly handle all four operation types (r, c, u, d)
  </action>
  <verify>
1. File exists at src/content/course/module-1/06-event-structure.mdx
2. Contains complete JSON event example with all fields annotated
3. Has operation type table (r, c, u, d)
4. Shows null handling for before/after
5. **Includes complete parse_cdc_event function that explicitly handles all operation types:**
   - 'r' (snapshot/read)
   - 'c' (create/INSERT)
   - 'u' (update/UPDATE)
   - 'd' (delete/DELETE)
6. Distinguishes snapshot vs streaming events
7. Run `npm run build` - no errors
  </verify>
  <done>
Lesson file exists with complete CDC event structure explanation. Contains operation type table for all four types (r, c, u, d). Includes complete parse_cdc_event function with explicit handling for snapshot (r), create (c), update (u), and delete (d) operations. Null handling patterns and module summary included.
  </done>
</task>

</tasks>

<verification>
After both tasks complete:
1. `ls src/content/course/module-1/` shows all 6 lesson files (01-06)
2. `npm run build` succeeds
3. Python code uses confluent-kafka (not kafka-python)
4. Lesson 05 has prominent kafka:9092 vs localhost:9092 warning
5. Event structure lesson shows complete envelope with all fields
6. parse_cdc_event function handles all operation types (r, c, u, d) explicitly
7. Module 1 feels complete as a learning unit
</verification>

<success_criteria>
- Two lesson files created with valid frontmatter
- Python consumer uses confluent-kafka with correct Docker hostname (kafka:9092)
- Prominent warning explains when to use localhost vs kafka hostname
- Event structure covers envelope format with before/after/source
- All four operation types (r, c, u, d) explained and handled in parse_cdc_event
- Null handling pattern demonstrated
- Reusable parse_cdc_event function provided with complete operation handling
- Module 1 concludes with summary and preview of Module 2
</success_criteria>

<output>
After completion, create `.planning/phases/05-module-1-foundations/05-03-SUMMARY.md`
</output>
