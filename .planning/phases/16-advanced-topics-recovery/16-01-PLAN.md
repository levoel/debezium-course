---
phase: 16-advanced-topics-recovery
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/content/course/08-module-8/13-recovery-procedures.mdx
autonomous: true

must_haves:
  truths:
    - "Learner can recover from binlog position loss using snapshot.mode=when_needed"
    - "Learner can recover from schema history topic corruption using snapshot.mode=recovery"
    - "Learner understands prevention vs recovery trade-offs (infinite retention, backups)"
    - "Learner can diagnose which recovery procedure applies based on error messages"
    - "Learner can backup and restore schema history topic using Kafka tools"
  artifacts:
    - path: "src/content/course/08-module-8/13-recovery-procedures.mdx"
      provides: "Recovery procedures lesson covering binlog loss and schema history corruption"
      min_lines: 500
      contains: ["snapshot.mode", "when_needed", "recovery", "retention.ms=-1", "kafka-console-consumer"]
  key_links:
    - from: "13-recovery-procedures.mdx"
      to: "06-schema-history-recovery.mdx"
      via: "references schema history concepts from Phase 13"
      pattern: "schema.history.internal"
    - from: "13-recovery-procedures.mdx"
      to: "03-binlog-retention-heartbeat.mdx"
      via: "references binlog retention configuration"
      pattern: "binlog_expire_logs_seconds"
---

<objective>
Create comprehensive recovery procedures lesson covering two critical failure scenarios: binlog position loss (purged binlogs) and schema history topic corruption

Purpose: Enable learners to diagnose and recover from the two most common MySQL CDC failures in production
Output: Lesson 13 in Module 8 with step-by-step recovery workflows, prevention strategies, and backup procedures
</objective>

<execution_context>
@/Users/levoely/.claude/get-shit-done/workflows/execute-plan.md
@/Users/levoely/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/16-advanced-topics-recovery/16-RESEARCH.md

# Prior phase context - schema history fundamentals
@.planning/phases/13-connector-setup-comparison/13-03-SUMMARY.md

# Existing lessons to reference
@src/content/course/08-module-8/06-schema-history-recovery.mdx
@src/content/course/08-module-8/03-binlog-retention-heartbeat.mdx
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create recovery procedures lesson</name>
  <files>src/content/course/08-module-8/13-recovery-procedures.mdx</files>
  <action>
Create comprehensive lesson on MySQL CDC recovery procedures with the following sections:

1. **Introduction** - Production reality: failures happen (binlogs purge, topics corrupt, connectors crash)
   - Two categories: data source issues (binlog) vs metadata issues (schema history)
   - Recovery philosophy: prevention cheaper than cure, but must know both
   - Reference lesson 06 for schema history fundamentals

2. **Recovery Decision Tree** (Mermaid flowchart)
   - Error message analysis -> diagnosis -> recovery path
   - "Connector requires binlog file X, but MySQL only has Y" -> binlog purged
   - "db history topic or its content is fully or partially missing" -> schema history issue
   - "Table X not found in schema history" -> schema mismatch

3. **Scenario 1: Binlog Position Loss** (from 16-RESEARCH.md Pattern 1, Pitfall 2)
   - Symptoms: Connector fails with "binlog file not available" error
   - Causes: binlog retention expired, manual PURGE BINARY LOGS, MySQL restart with aggressive retention
   - Diagnosis steps:
     * Check available binlogs: `SHOW BINARY LOGS`
     * Check offset topic for required position
     * Compare timestamps
   - Recovery procedure (step-by-step with code):
     * Option A: `snapshot.mode=when_needed` - automatic snapshot trigger
     * Option B: Delete offset, restart with `snapshot.mode=initial` (fresh start)
   - Include complete connector config JSON for Option A
   - Warning callout: when_needed may cause duplicate events during snapshot

4. **Scenario 2: Schema History Topic Corruption** (from 16-RESEARCH.md Pattern 2, Pitfall 1, 4)
   - Symptoms: "db history topic partially missing", deserialization errors
   - Causes: Kafka retention purged topic, accidental deletion, broker failure
   - Diagnosis steps:
     * Check topic exists: `kafka-topics.sh --describe`
     * Check topic retention config
     * Count messages vs expected DDL count
   - Recovery procedure (step-by-step):
     * Option A: `snapshot.mode=recovery` - rebuild from current schema
     * CRITICAL prerequisite: verify no DDL since last offset (Pitfall 4)
     * DDL verification: `SHOW BINLOG EVENTS IN 'file' FROM position` looking for CREATE/ALTER/DROP
     * Option B: Restore from backup (if backup exists)
     * Option C: Fresh start with new connector name (if DDL happened)
   - Danger callout: recovery mode assumes no schema changes - silent data corruption if wrong

5. **Prevention: Defense in Depth** (from 16-RESEARCH.md Common Pitfalls)
   - Layer 1: Infinite retention on schema history topic
     * Command: `kafka-configs.sh --alter --add-config retention.ms=-1,retention.bytes=-1`
     * Why both: retention.ms AND retention.bytes (broker-level override protection)
   - Layer 2: Adequate binlog retention
     * Formula: Max(snapshot_duration, expected_downtime) + 50% safety margin
     * For 500GB table with 6-hour snapshot: minimum 9+ hours retention
     * Recommendation: 14+ days for production
   - Layer 3: Regular backups
     * Schema history topic backup script (from 16-RESEARCH.md Pattern 5)
     * Backup frequency: daily for active databases, before schema changes
   - Layer 4: Monitoring
     * Alert on schema history topic message count drop
     * Alert on offset age vs oldest binlog

6. **Schema History Backup and Restore** (from 16-RESEARCH.md Pattern 5)
   - Backup command with kafka-console-consumer
   - Restore command with kafka-console-producer
   - Automation script with cron schedule
   - Tip callout: backup before connector upgrades

7. **Recovery Time Estimates** (help learners plan)
   - when_needed snapshot: proportional to data size (1TB = ~hours)
   - recovery mode: seconds to minutes (reads current schema only)
   - Restore from backup: seconds to minutes
   - Fresh connector: snapshot time + reprocessing downstream

8. **Common Mistakes During Recovery** (from 16-RESEARCH.md Anti-Patterns)
   - Using recovery mode when DDL happened -> data corruption
   - Forgetting to change snapshot.mode back after recovery
   - Not verifying recovery before resuming downstream consumers

9. **Hands-on Exercise: Simulated Recovery**
   - Exercise 1: Simulate binlog purge and recover
     * Stop connector, purge binlogs manually, try restart, observe error
     * Configure when_needed mode, restart, observe automatic snapshot
   - Exercise 2: Backup and restore schema history topic
     * Create backup with kafka-console-consumer
     * Delete topic (or some messages)
     * Restore from backup
     * Verify connector starts successfully

10. **Summary Checklist**
    - Pre-production: Configure retention, set up backups, configure monitoring
    - During incident: Identify error type, check for DDL, choose recovery path
    - Post-recovery: Verify data, reset snapshot.mode, update alerts

Use callout components:
- type="danger": recovery mode with DDL changes = silent data corruption
- type="warning": when_needed may produce duplicates during snapshot
- type="tip": backup schema history before connector upgrades
- type="note": prevention is cheaper than recovery

Include Mermaid diagrams:
- Recovery decision flowchart
- Prevention layers diagram (defense in depth)

Follow established patterns:
- Russian text / English code
- Pedagogical structure matching prior Module 8 lessons
- Use Callout component from src/components/Callout.tsx
- Use relative imports: `import Callout from '../../../components/Callout.tsx'`
- Use Mermaid component: `import { Mermaid } from '../../../components/Mermaid.tsx'`

Frontmatter:
```yaml
title: "Recovery Procedures: Binlog Loss and Schema History Corruption"
description: "Step-by-step recovery from binlog position loss and schema history topic corruption"
---
```
  </action>
  <verify>
Build passes: `cd "/Users/levoely/debezium course" && npm run build`
File exists with min 500 lines
Contains snapshot.mode, when_needed, recovery, retention.ms=-1, kafka-console-consumer keywords
Mermaid diagrams render correctly
  </verify>
  <done>
Lesson 13 created covering both recovery scenarios with decision tree, step-by-step procedures, prevention strategies, backup/restore commands, and hands-on exercises
  </done>
</task>

</tasks>

<verification>
1. Lesson file exists at correct path with proper frontmatter
2. Build passes without errors
3. Both recovery scenarios covered with step-by-step procedures
4. Prevention section covers infinite retention, binlog retention, backups
5. Hands-on exercises are executable in Docker lab environment
6. References prior lessons (06-schema-history-recovery, 03-binlog-retention-heartbeat) appropriately
</verification>

<success_criteria>
- Learner can diagnose which recovery procedure applies based on error message
- Both recovery procedures (when_needed, recovery) explained with complete configs
- Prevention strategies prioritized (defense in depth)
- Backup/restore commands are copy-paste ready
- Exercises allow learners to practice recovery in safe environment
</success_criteria>

<output>
After completion, create `.planning/phases/16-advanced-topics-recovery/16-01-SUMMARY.md`
</output>
