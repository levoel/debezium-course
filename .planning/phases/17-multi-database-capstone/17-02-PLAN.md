---
phase: 17-multi-database-capstone
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/content/course/07-module-7/05-multi-database-configuration.mdx
autonomous: true

must_haves:
  truths:
    - "Learner can configure PostgreSQL connector for multi-database deployment"
    - "Learner can configure MySQL connector with unique server.id and schema history topic"
    - "Learner can configure unified topic naming scheme"
    - "Learner can create PyFlink consumer processing events from both databases"
  artifacts:
    - path: "src/content/course/07-module-7/05-multi-database-configuration.mdx"
      provides: "Complete configuration guide for multi-database CDC pipeline"
      min_lines: 350
      contains: "mysql-outbox-connector"
  key_links:
    - from: "05-multi-database-configuration.mdx"
      to: "PostgreSQL connector config"
      via: "JSON configuration example"
      pattern: "PostgresConnector"
    - from: "05-multi-database-configuration.mdx"
      to: "MySQL connector config"
      via: "JSON configuration example"
      pattern: "MySqlConnector"
    - from: "05-multi-database-configuration.mdx"
      to: "PyFlink consumer"
      via: "Python code example"
      pattern: "UNION ALL"
---

<objective>
Create the multi-database configuration lesson that provides complete code examples for deploying both connectors and a unified PyFlink consumer.

Purpose: Give learners ready-to-use configuration for their capstone extension. This is the hands-on implementation lesson after the architecture concepts.

Output: `src/content/course/07-module-7/05-multi-database-configuration.mdx` (~400-500 lines)
</objective>

<execution_context>
@/Users/levoely/.claude/get-shit-done/workflows/execute-plan.md
@/Users/levoely/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/17-multi-database-capstone/17-RESEARCH.md

# Existing capstone for style consistency
@src/content/course/07-module-7/02-architecture-deliverables.mdx

# Prior connector examples
@src/content/course/07-module-7/01-capstone-overview.mdx

# Components
@src/components/Mermaid.tsx
@src/components/Callout.tsx
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create configuration lesson structure with connector configs</name>
  <files>src/content/course/07-module-7/05-multi-database-configuration.mdx</files>
  <action>
Create the lesson file with the following structure:

**Frontmatter:**
```yaml
---
title: "Multi-Database Configuration"
description: "Конфигурация PostgreSQL и MySQL коннекторов для multi-database CDC pipeline"
order: 5
difficulty: "advanced"
estimatedTime: 45
topics: ["Configuration", "PostgreSQL", "MySQL", "PyFlink", "Kafka Connect"]
prerequisites: ["module-7/04-multi-database-architecture"]
---
```

**Imports:**
```tsx
import { Mermaid } from '../../../components/Mermaid.tsx';
import Callout from '../../../components/Callout.tsx';
```

**Content sections:**

1. **Introduction**
   - Reference architecture lesson (04-multi-database-architecture)
   - Goal: Configure both connectors and unified consumer

2. **MySQL Outbox Table Schema**
   - SQL DDL for MySQL outbox table (parallel to PostgreSQL version)
   - Note differences: UUID() syntax, JSON type (not JSONB), no REPLICA IDENTITY equivalent
   - Include Callout: "MySQL binlog with ROW format captures full row data automatically"

3. **PostgreSQL Connector Configuration**
   - Full JSON config with comments
   - Key parameters: database.server.name, slot.name, table.include.list
   - Outbox Event Router SMT configuration
   - Topic naming: `outbox.event.postgres.${routedByValue}`

4. **MySQL Connector Configuration**
   - Full JSON config with comments
   - Key MySQL-specific parameters:
     - database.server.id: 184054 (unique, from registry pattern)
     - schema.history.internal.kafka.topic: "schema-changes.mysql-outbox"
     - schema.history.internal.kafka.bootstrap.servers
   - Outbox Event Router SMT (same as PostgreSQL)
   - Topic naming: `outbox.event.mysql.${routedByValue}`

   - Use Callout for critical requirements:
     - "CRITICAL: schema.history.internal.kafka.topic must be unique per MySQL connector"
     - "database.server.id must be unique (not conflict with MySQL server's server_id)"

5. **Deploying Both Connectors**
   - curl commands to deploy via Kafka Connect REST API
   - Verification commands: `curl http://localhost:8083/connectors | jq`
   - Expected output: `["postgres-outbox-connector", "mysql-outbox-connector"]`

**Russian text / English code convention applies.**
Include complete, copy-paste ready JSON configs.
  </action>
  <verify>
File exists at src/content/course/07-module-7/05-multi-database-configuration.mdx
File contains valid MDX frontmatter with order: 5
File contains MySQL outbox table DDL
File contains PostgreSQL connector JSON config
File contains MySQL connector JSON config with server.id and schema.history.internal.kafka.topic
File contains curl deployment commands
  </verify>
  <done>
Lesson created with connector configuration section covering both PostgreSQL and MySQL with complete JSON examples
  </done>
</task>

<task type="auto">
  <name>Task 2: Add PyFlink unified consumer and monitoring sections</name>
  <files>src/content/course/07-module-7/05-multi-database-configuration.mdx</files>
  <action>
Add the remaining sections to complete the lesson:

6. **PyFlink Multi-Source Consumer**
   - Complete Python code for unified consumer
   - Two Kafka sources: orders_postgres_cdc, orders_mysql_cdc
   - UNION ALL pattern for merging streams
   - source_database tracking column for traceability

```python
from pyflink.table import EnvironmentSettings, TableEnvironment

env_settings = EnvironmentSettings.in_streaming_mode()
table_env = TableEnvironment.create(env_settings)

# PostgreSQL source
postgres_source_ddl = """
    CREATE TABLE orders_postgres_cdc (
        order_id BIGINT,
        customer_id BIGINT,
        total_amount DECIMAL(10, 2),
        created_at TIMESTAMP(3),
        PRIMARY KEY (order_id) NOT ENFORCED
    ) WITH (
        'connector' = 'kafka',
        'topic' = 'outbox.event.postgres.orders',
        'properties.bootstrap.servers' = 'kafka:9092',
        'format' = 'debezium-json',
        'debezium-json.schema-include' = 'false'
    )
"""

# MySQL source
mysql_source_ddl = """
    CREATE TABLE orders_mysql_cdc (
        order_id BIGINT,
        customer_id BIGINT,
        total_amount DECIMAL(10, 2),
        created_at TIMESTAMP(3),
        PRIMARY KEY (order_id) NOT ENFORCED
    ) WITH (
        'connector' = 'kafka',
        'topic' = 'outbox.event.mysql.orders',
        'properties.bootstrap.servers' = 'kafka:9092',
        'format' = 'debezium-json',
        'debezium-json.schema-include' = 'false'
    )
"""

# Unified view with source tracking
unified_view_sql = """
    CREATE VIEW unified_orders AS
    SELECT
        order_id,
        customer_id,
        total_amount,
        created_at,
        'postgresql' AS source_database
    FROM orders_postgres_cdc
    UNION ALL
    SELECT
        order_id,
        customer_id,
        total_amount,
        created_at,
        'mysql' AS source_database
    FROM orders_mysql_cdc
"""
```

7. **Monitoring Multi-Database Connectors**
   - Prometheus queries for both connector types
   - Key metrics:
     - PostgreSQL: WAL lag bytes from pg_replication_slots
     - MySQL: MilliSecondsBehindSource JMX metric
   - Include Mermaid diagram showing monitoring architecture

8. **Verification Checklist**
   - Checklist for verifying multi-database setup works:
     - [ ] Both connectors RUNNING
     - [ ] PostgreSQL topic receives events
     - [ ] MySQL topic receives events
     - [ ] PyFlink consumer processes both streams
     - [ ] source_database column correctly identifies origin

9. **Common Issues and Troubleshooting**
   - Table of common issues:
     - "Connector fails with duplicate server_id" -> Check MySQL server's server_id
     - "Schema mismatch errors" -> Unique schema.history.internal.kafka.topic per connector
     - "Topic naming conflicts" -> Unique database.server.name per connector
     - "PyFlink parsing errors" -> Ensure format = 'debezium-json'

10. **Key Takeaways**
    - Summary of critical configuration requirements
    - Links to next lesson (checklist extension) and prior architecture lesson

Ensure code blocks have syntax highlighting (json, sql, python).
Use Callout for warnings and tips throughout.
  </action>
  <verify>
File contains PyFlink UNION ALL consumer code
File contains monitoring section with metrics
File contains verification checklist
File contains troubleshooting table
File is at least 350 lines
Run: npm run build (should pass)
  </verify>
  <done>
Complete multi-database configuration lesson with connector configs, PyFlink consumer, monitoring guidance, and troubleshooting
  </done>
</task>

</tasks>

<verification>
After completing both tasks:
1. File exists: src/content/course/07-module-7/05-multi-database-configuration.mdx
2. Frontmatter valid with order: 5
3. Contains MySQL outbox table DDL
4. Contains complete PostgreSQL connector JSON config
5. Contains complete MySQL connector JSON config with server.id and schema.history.internal.kafka.topic
6. Contains PyFlink UNION ALL consumer code with source_database tracking
7. Contains monitoring section
8. Contains verification checklist
9. Contains troubleshooting table
10. Build passes: `npm run build`
</verification>

<success_criteria>
- Learner can copy-paste connector configs and deploy both connectors
- Learner understands MySQL-specific requirements (server.id, schema history topic)
- Learner can implement PyFlink consumer processing events from both databases
- Learner knows how to monitor both connector types
- Learner has troubleshooting reference for common issues
</success_criteria>

<output>
After completion, create `.planning/phases/17-multi-database-capstone/17-02-SUMMARY.md`
</output>
