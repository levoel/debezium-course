---
phase: 09-module-5-data-engineering
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/content/course/05-module-5/03-pyflink-cdc-connector.mdx
  - src/content/course/05-module-5/04-pyflink-stateful-processing.mdx
autonomous: true

must_haves:
  truths:
    - "Student can create PyFlink TableEnvironment in streaming mode"
    - "Student can define Kafka source table with CDC schema using SQL DDL"
    - "Student can extract before/after fields from Debezium envelope in PyFlink"
    - "Student can implement tumbling, sliding, and session windows"
    - "Student can configure watermarks for late data handling"
    - "Student can perform temporal joins between CDC streams"
  artifacts:
    - path: "src/content/course/05-module-5/03-pyflink-cdc-connector.mdx"
      provides: "PyFlink CDC connector setup lesson"
      min_lines: 180
      contains: "TableEnvironment"
    - path: "src/content/course/05-module-5/04-pyflink-stateful-processing.mdx"
      provides: "PyFlink stateful processing lesson"
      min_lines: 220
      contains: "TUMBLE"
  key_links:
    - from: "03-pyflink-cdc-connector.mdx"
      to: "Kafka topic"
      via: "connector configuration"
      pattern: "connector.*=.*kafka"
    - from: "04-pyflink-stateful-processing.mdx"
      to: "event_time watermark"
      via: "WATERMARK FOR"
      pattern: "WATERMARK.*FOR.*event_time"
---

<objective>
Create PyFlink lessons covering CDC connector setup and stateful stream processing with windows, aggregations, and temporal joins.

Purpose: PyFlink is the Python API for Apache Flink, enabling complex stream processing. These lessons cover MOD5-03 (connector setup) and MOD5-04 (stateful processing) - critical skills for building real-time data pipelines.

Output: Two MDX lesson files covering PyFlink Table API for CDC processing with Russian explanatory text and English code.
</objective>

<execution_context>
@/Users/levoely/.claude/get-shit-done/workflows/execute-plan.md
@/Users/levoely/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-module-5-data-engineering/09-RESEARCH.md

# Reference for lesson structure
@src/content/course/04-module-4/01-smt-overview.mdx
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create PyFlink CDC Connector Lesson</name>
  <files>src/content/course/05-module-5/03-pyflink-cdc-connector.mdx</files>
  <action>
Create lesson covering MOD5-03 (PyFlink CDC connector setup).

**Frontmatter:**
- title: "PyFlink CDC Connector — настройка и использование"
- description: "Подключение PyFlink к Kafka для обработки CDC событий"
- order: 3
- difficulty: "advanced"
- estimatedTime: 40
- topics: ["PyFlink", "Flink", "Stream Processing", "Kafka Connector", "Table API"]
- prerequisites: ["module-5/02-pandas-integration"]

**Content structure (Russian explanatory text, English code):**

1. **Что такое PyFlink?** (~150 words)
   - Python API for Apache Flink
   - Difference from pandas: streaming vs batch, distributed vs single-machine
   - When to use: continuous processing, complex event processing, stateful operations
   - Mermaid diagram showing PyFlink architecture

2. **Table API vs DataStream API** (~100 words)
   - Table API (SQL-like) recommended for CDC
   - Better for structured data with schema
   - Easier to understand and maintain

3. **Настройка окружения** (~150 words)
   - Python 3.11-3.12 requirement (from RESEARCH.md)
   - `pip install apache-flink==2.2.0`
   - Note: PyFlink requires Java 11+ runtime
   - JupyterLab already has dependencies

4. **Создание TableEnvironment** (~100 words)
   - `EnvironmentSettings.in_streaming_mode()`
   - `TableEnvironment.create()`
   - Code example

5. **Определение Kafka Source Table** (~300 words)
   - SQL DDL for CDC source table
   - Schema mapping for Debezium envelope
   - `payload ROW<before ROW<...>, after ROW<...>, op STRING, ts_ms BIGINT>`
   - Connector options: bootstrap.servers, topic, group.id, format
   - `json.ignore-parse-errors` for malformed messages
   - Full code example from RESEARCH.md Pattern 3

6. **Извлечение текущего состояния** (~150 words)
   - CREATE VIEW for extracting after state
   - Filter by operation type (c, u, r)
   - Convert ts_ms to TIMESTAMP with TO_TIMESTAMP_LTZ
   - Code example for orders_current view

7. **Обработка ошибок десериализации** (~100 words)
   - `json.fail-on-missing-field=false`
   - `json.ignore-parse-errors=true`
   - Why: schema changes, malformed messages
   - Reference RESEARCH.md Pitfall 7

8. **Лабораторная работа: PyFlink CDC Source** (~100 words)
   - Exercise: create Kafka source table for orders topic
   - Query current order state
   - Print results with `result.execute().print()`

Include Mermaid diagram showing PyFlink data flow.
Reference PyFlink 2.2.0 documentation for syntax.
  </action>
  <verify>
- File exists at src/content/course/05-module-5/03-pyflink-cdc-connector.mdx
- Frontmatter has all required fields
- Contains TableEnvironment creation code
- Contains SQL DDL for Kafka source table
- Contains Debezium envelope schema (payload.before, payload.after)
- Contains connector options (bootstrap.servers, format, topic)
- Contains error handling configuration
- Mermaid diagram present
- Russian explanatory text, English code
  </verify>
  <done>Lesson 03-pyflink-cdc-connector.mdx exists with Table API setup, Kafka connector configuration, and lab exercise</done>
</task>

<task type="auto">
  <name>Task 2: Create PyFlink Stateful Processing Lesson</name>
  <files>src/content/course/05-module-5/04-pyflink-stateful-processing.mdx</files>
  <action>
Create lesson covering MOD5-04 (stateful stream processing in PyFlink).

**Frontmatter:**
- title: "Stateful Stream Processing в PyFlink"
- description: "Оконные агрегации, temporal joins и управление состоянием"
- order: 4
- difficulty: "advanced"
- estimatedTime: 50
- topics: ["PyFlink", "Windows", "Aggregations", "Joins", "Watermarks", "State"]
- prerequisites: ["module-5/03-pyflink-cdc-connector"]

**Content structure (Russian explanatory text, English code):**

1. **Зачем нужно состояние в потоковой обработке?** (~150 words)
   - Streaming differs from batch: data arrives continuously
   - State needed for: aggregations over time, joins, deduplication
   - Mermaid diagram showing stateful processing concept

2. **Watermarks и обработка поздних данных** (~200 words)
   - What is watermark: progress marker for event time
   - `WATERMARK FOR event_time AS event_time - INTERVAL '5' SECONDS`
   - Late data handling: allowed lateness vs drop
   - Why important for CDC: network delays, replication lag

3. **Tumbling Windows (неперекрывающиеся окна)** (~200 words)
   - Fixed-size, non-overlapping time windows
   - `TUMBLE(event_time, INTERVAL '5' MINUTES)`
   - Use case: hourly/daily aggregations
   - Full code example from RESEARCH.md Pattern 4 (tumbling)
   - SQL with GROUP BY TUMBLE, COUNT, SUM, AVG

4. **Sliding Windows (скользящие окна)** (~150 words)
   - Overlapping windows with slide interval
   - `HOP(event_time, INTERVAL '5' MINUTES, INTERVAL '10' MINUTES)`
   - Use case: moving averages, rolling metrics
   - Code example from RESEARCH.md Pattern 4 (sliding)

5. **Session Windows (сессионные окна)** (~150 words)
   - Dynamic windows based on inactivity gap
   - `SESSION(event_time, INTERVAL '30' MINUTES)`
   - Use case: user session analysis, clickstream
   - Code example from RESEARCH.md Pattern 4 (session)

6. **Temporal Joins (временные соединения)** (~250 words)
   - Join stream with versioned dimension table
   - `FOR SYSTEM_TIME AS OF` syntax
   - Use case: enrich orders with product info at order time
   - Full code example from RESEARCH.md (Temporal Join)
   - Orders CDC joined with Products CDC

7. **Управление размером состояния** (~150 words)
   - Warning about Pitfall 5 (state size explosion)
   - Configure state TTL
   - Monitor state size metrics
   - RocksDB state backend for large state

8. **Лабораторная работа: Real-Time Dashboard** (~100 words)
   - Exercise: create 5-minute tumbling window aggregation
   - Calculate: orders per customer, total revenue
   - Create sliding window for moving average order value

Include Mermaid diagrams for window types (tumbling vs sliding vs session).
Reference RESEARCH.md for all code examples.
  </action>
  <verify>
- File exists at src/content/course/05-module-5/04-pyflink-stateful-processing.mdx
- Frontmatter has all required fields
- Contains WATERMARK configuration
- Contains TUMBLE window example with GROUP BY
- Contains HOP (sliding) window example
- Contains SESSION window example
- Contains temporal join with FOR SYSTEM_TIME AS OF
- Contains state management warning
- Mermaid diagrams for window types
- Russian explanatory text, English code
  </verify>
  <done>Lesson 04-pyflink-stateful-processing.mdx exists with window types, temporal joins, state management, and lab exercise</done>
</task>

</tasks>

<verification>
- [ ] Both lesson files exist in src/content/course/05-module-5/
- [ ] Lesson order field is sequential (3, 4)
- [ ] Prerequisites chain correctly
- [ ] PyFlink SQL syntax is correct (2.2.0 compatible)
- [ ] Window examples cover all three types
- [ ] Temporal join syntax is correct
- [ ] Mermaid diagrams render properly
- [ ] Russian language for explanatory text
- [ ] English for code, config, technical terms
</verification>

<success_criteria>
1. Student can create PyFlink TableEnvironment and configure Kafka source
2. Student can define CDC schema mapping for Debezium envelope
3. Student can implement tumbling, sliding, and session windows
4. Student can configure watermarks for late data handling
5. Student can perform temporal joins between CDC streams
6. Student understands state size management concerns
</success_criteria>

<output>
After completion, create `.planning/phases/09-module-5-data-engineering/09-02-SUMMARY.md`
</output>
