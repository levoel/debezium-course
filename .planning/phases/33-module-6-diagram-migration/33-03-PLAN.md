---
phase: 33-module-6-diagram-migration
plan: 03
type: execute
wave: 1
depends_on: []
files_modified:
  - src/components/diagrams/module6/PysparkStreamingDiagrams.tsx
  - src/components/diagrams/module6/EtlEltPatternDiagrams.tsx
autonomous: true

must_haves:
  truths:
    - "User can see PyFlink vs PySpark philosophy comparison"
    - "User can see Structured Streaming conceptual model"
    - "User can see ETL vs ELT architecture comparison"
    - "User can see CDC to Data Lake flow with multiple output types"
    - "All diagrams have Russian tooltips with data lake terminology"
  artifacts:
    - path: "src/components/diagrams/module6/PysparkStreamingDiagrams.tsx"
      provides: "4 diagrams for lesson 05 (PySpark philosophy, streaming model)"
      exports: ["PyflinkVsPysparkComparisonDiagram", "StructuredStreamingConceptDiagram", "PysparkWatermarkDiagram", "MicroBatchVsContinuousDiagram"]
    - path: "src/components/diagrams/module6/EtlEltPatternDiagrams.tsx"
      provides: "5 diagrams for lesson 06 (ETL, ELT, data lake patterns)"
      exports: ["TraditionalEtlDiagram", "ModernEltDiagram", "CdcToDataLakeDiagram", "AppendOnlyHistoryDiagram", "OperationSeparationDiagram"]
  key_links:
    - from: "src/components/diagrams/module6/PysparkStreamingDiagrams.tsx"
      to: "src/components/diagrams/primitives/*"
      via: "import FlowNode, Arrow, DiagramContainer, DiagramTooltip"
      pattern: "import.*from.*primitives"
    - from: "src/components/diagrams/module6/EtlEltPatternDiagrams.tsx"
      to: "src/components/diagrams/primitives/*"
      via: "import FlowNode, Arrow, DiagramContainer, DiagramTooltip"
      pattern: "import.*from.*primitives"
---

<objective>
Create interactive glass diagram components for Module 6 lessons 05-06 (PySpark & ETL/ELT).

Purpose: Replace 9 Mermaid diagrams in PySpark Structured Streaming and ETL/ELT patterns lessons with interactive glass components. Focus on data lake architecture and batch/streaming unification concepts.

Output:
- PysparkStreamingDiagrams.tsx with 4 diagrams
- EtlEltPatternDiagrams.tsx with 5 diagrams
</objective>

<execution_context>
@/Users/levoely/.claude/get-shit-done/workflows/execute-plan.md
@/Users/levoely/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/33-module-6-diagram-migration/33-RESEARCH.md

# Reference primitives
@src/components/diagrams/primitives/FlowNode.tsx
@src/components/diagrams/primitives/Arrow.tsx
@src/components/diagrams/primitives/DiagramContainer.tsx
@src/components/diagrams/primitives/Tooltip.tsx

# Reference patterns from Module 5
@src/components/diagrams/module5/OutboxPatternDiagrams.tsx

# Source MDX files
@src/content/course/06-module-6/05-pyspark-structured-streaming.mdx
@src/content/course/06-module-6/06-etl-elt-patterns.mdx
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create PysparkStreamingDiagrams.tsx</name>
  <files>src/components/diagrams/module6/PysparkStreamingDiagrams.tsx</files>
  <action>
Create diagram component file with 4 diagrams:

**1. PyflinkVsPysparkComparisonDiagram** (side-by-side philosophy)
- Left: "PyFlink Philosophy" (blue container)
  - 4 nodes vertically:
    - "Pure Streaming - Event-by-event processing"
    - "Low Latency - Millisecond response"
    - "Exactly-Once - Strong guarantees"
    - "Complex CEP - Pattern matching"
- Right: "PySpark Philosophy" (purple container)
  - 4 nodes vertically:
    - "Unified Batch + Streaming - Same API for both"
    - "Micro-batching - Second-scale latency"
    - "Data Lake Native - Parquet, Delta, Iceberg"
    - "ML Integration - Spark MLlib ecosystem"
- Tooltips explaining each point in Russian

**2. StructuredStreamingConceptDiagram** (horizontal flow)
- Three containers in horizontal flow:
  - "Input Stream" (blue): "Kafka Topic - Unbounded table"
  - "Spark Processing" (purple): "DataFrame API" -> "Trigger - Micro-batch intervals"
  - "Output Stream" (emerald): "Sink - Parquet, Kafka, Console"
- Arrows: "readStream" between Input and Processing, "writeStream" between Processing and Output
- Tooltips:
  - "Kafka topic рассматривается как unbounded table"
  - "DataFrame API работает одинаково для batch и streaming"
  - "Trigger определяет частоту micro-batch обработки"

**3. PysparkWatermarkDiagram** (similar to PyFlink watermark)
- Show late event handling with watermark threshold
- Timeline with events and watermark progress
- Dropped late event indicator (rose color)
- Text: "PySpark отбрасывает события позже watermark threshold"

**4. MicroBatchVsContinuousDiagram** (comparison)
- Two side-by-side containers:
  - "Micro-batch (Default)" (amber):
    - "Каждый batch как отдельная транзакция"
    - "Latency ~100ms+"
    - "Стабильный и проверенный режим"
  - "Continuous (Experimental)" (rose):
    - "Event-by-event processing"
    - "Latency ~1ms"
    - "Экспериментальный режим"
- Note: "Рекомендация: используйте micro-batch для CDC"
  </action>
  <verify>
- File exists at src/components/diagrams/module6/PysparkStreamingDiagrams.tsx
- Exports all 4 diagram components
- No TypeScript errors
  </verify>
  <done>
4 diagrams for PySpark streaming lesson created: PyFlink vs PySpark comparison, Structured Streaming concept, watermark handling, micro-batch vs continuous
  </done>
</task>

<task type="auto">
  <name>Task 2: Create EtlEltPatternDiagrams.tsx</name>
  <files>src/components/diagrams/module6/EtlEltPatternDiagrams.tsx</files>
  <action>
Create diagram component file with 5 diagrams:

**1. TraditionalEtlDiagram** (horizontal flow)
- Flow: "Source DB (PostgreSQL)" -> "Staging Area" -> "ETL Engine (Filter, aggregate, join, clean)" -> "Data Warehouse (Clean, aggregated)"
- ETL Engine highlighted in amber
- DW highlighted in emerald
- Tooltips:
  - "Extract: читаем данные из source"
  - "Transform: применяем бизнес-логику"
  - "Load: загружаем готовые данные"

**2. ModernEltDiagram** (horizontal flow)
- Flow: "Source DB (PostgreSQL)" -> "CDC Stream (Debezium)" -> "Data Lake (Parquet, Delta)" -> "Data Warehouse (dbt, Spark SQL)"
- Data Lake highlighted in blue
- DW highlighted in emerald
- Tooltips:
  - "Extract: CDC захватывает изменения"
  - "Load: сырые events в data lake"
  - "Transform: трансформации в warehouse (SQL, dbt)"

**3. CdcToDataLakeDiagram** (multi-layer with three outputs)
- Top: "Transactional Database - Customer changes (INSERT, UPDATE, DELETE)"
- Middle arrows pointing to bottom layer
- Bottom: "Data Lake (Parquet)" with three sub-containers:
  - "Raw CDC events (append-only log)" (blue)
  - "Latest snapshot (current state)" (emerald)
  - "Change history (audit trail)" (purple)
- Tooltips explaining each layer purpose

**4. AppendOnlyHistoryDiagram** (vertical flow showing metadata)
- Source -> Debezium -> Kafka -> PySpark -> Parquet sink
- Show metadata columns being added:
  - "_operation (c, u, d, r)"
  - "_cdc_timestamp"
  - "_processed_at"
  - "_source_db"
  - "_source_table"
- Tooltips explaining each metadata column

**5. OperationSeparationDiagram** (branching flow)
- Input: "CDC Stream"
- Three branches based on operation type:
  - "op='c' (INSERT)" -> "inserts_parquet/"
  - "op='u' (UPDATE)" -> "updates_parquet/"
  - "op='d' (DELETE)" -> "deletes_parquet/"
- Color coding: emerald for inserts, amber for updates, rose for deletes
- Tooltip: "Разделение по типу операции упрощает downstream обработку"

All tooltips in Russian using data lake terminology from RESEARCH.md glossary.
  </action>
  <verify>
- File exists at src/components/diagrams/module6/EtlEltPatternDiagrams.tsx
- Exports all 5 diagram components
- No TypeScript errors
  </verify>
  <done>
5 diagrams for ETL/ELT patterns lesson created: traditional ETL, modern ELT, CDC to data lake, append-only history with metadata, operation separation
  </done>
</task>

</tasks>

<verification>
- Both component files exist in src/components/diagrams/module6/
- All 9 diagrams export correctly
- TypeScript compiles without errors
- ETL vs ELT comparison shows transformation location difference
- Data lake diagrams show multiple output types (raw, snapshot, history)
- Operation separation shows color-coded paths
- All tooltips are in Russian with correct data lake terminology
- Dev server renders diagrams without errors
</verification>

<success_criteria>
- 9 diagram components created for Module 6 lessons 05-06
- PySpark vs PyFlink philosophy clearly differentiated
- ETL vs ELT patterns show transform timing difference
- Data lake patterns show raw/snapshot/history split
- Metadata columns visualized for append-only pattern
- Operation separation shows branching by op type
- All Russian tooltips use consistent data lake terminology
- TypeScript compiles cleanly
</success_criteria>

<output>
After completion, create `.planning/phases/33-module-6-diagram-migration/33-03-SUMMARY.md`
</output>
