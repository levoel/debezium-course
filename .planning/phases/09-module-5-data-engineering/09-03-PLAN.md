---
phase: 09-module-5-data-engineering
plan: 03
type: execute
wave: 1
depends_on: []
files_modified:
  - src/content/course/05-module-5/05-pyspark-structured-streaming.mdx
  - src/content/course/05-module-5/06-etl-elt-patterns.mdx
autonomous: true

must_haves:
  truths:
    - "Student can configure PySpark to read CDC events from Kafka"
    - "Student can define Debezium schema using StructType"
    - "Student can parse JSON CDC events with from_json"
    - "Student can configure checkpoints for fault tolerance"
    - "Student can implement watermark-based aggregations"
    - "Student can design CDC-to-data-lake ETL pipeline"
    - "Student can separate INSERT/UPDATE/DELETE operations for different sinks"
  artifacts:
    - path: "src/content/course/05-module-5/05-pyspark-structured-streaming.mdx"
      provides: "PySpark Structured Streaming lesson"
      min_lines: 220
      contains: "readStream"
    - path: "src/content/course/05-module-5/06-etl-elt-patterns.mdx"
      provides: "ETL/ELT patterns lesson"
      min_lines: 200
      contains: "writeStream"
  key_links:
    - from: "05-pyspark-structured-streaming.mdx"
      to: "Kafka source"
      via: "readStream.format('kafka')"
      pattern: "format.*kafka.*subscribe"
    - from: "06-etl-elt-patterns.mdx"
      to: "Data lake sink"
      via: "writeStream to parquet"
      pattern: "writeStream.*format.*parquet"
---

<objective>
Create PySpark Structured Streaming lesson and ETL/ELT patterns lesson for CDC data processing at scale.

Purpose: PySpark is essential for large-scale data engineering. These lessons cover MOD5-05 (Structured Streaming) and MOD5-06 (ETL/ELT patterns) - enabling students to build production data pipelines that handle CDC events at scale.

Output: Two MDX lesson files covering PySpark streaming from Kafka and ETL/ELT patterns with Russian explanatory text and English code.
</objective>

<execution_context>
@/Users/levoely/.claude/get-shit-done/workflows/execute-plan.md
@/Users/levoely/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-module-5-data-engineering/09-RESEARCH.md

# Reference for lesson structure
@src/content/course/04-module-4/01-smt-overview.mdx
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create PySpark Structured Streaming Lesson</name>
  <files>src/content/course/05-module-5/05-pyspark-structured-streaming.mdx</files>
  <action>
Create lesson covering MOD5-05 (PySpark Structured Streaming with Kafka).

**Frontmatter:**
- title: "PySpark Structured Streaming с Kafka"
- description: "Потоковая обработка CDC событий с использованием Apache Spark"
- order: 5
- difficulty: "advanced"
- estimatedTime: 45
- topics: ["PySpark", "Spark", "Structured Streaming", "Kafka", "Watermarks"]
- prerequisites: ["module-5/04-pyflink-stateful-processing"]

**Content structure (Russian explanatory text, English code):**

1. **PySpark vs PyFlink: когда что использовать** (~150 words)
   - PySpark: batch + streaming unified, ML integration, data lake native
   - PyFlink: lower latency, stronger exactly-once, complex event processing
   - Decision table with use cases
   - Mermaid diagram comparing architectures

2. **Structured Streaming основы** (~150 words)
   - DataFrame API for streaming
   - Micro-batch vs continuous processing
   - Checkpoints for fault tolerance

3. **Настройка SparkSession** (~100 words)
   - `SparkSession.builder.appName().config().getOrCreate()`
   - Required config: `spark.sql.streaming.checkpointLocation`
   - Note about local mode vs cluster

4. **Определение CDC схемы** (~200 words)
   - StructType/StructField for Debezium envelope
   - Nested schema for before/after
   - Full schema definition from RESEARCH.md Pattern 5
   - Note: Schema must match exactly or parsing fails

5. **Чтение из Kafka** (~200 words)
   - `spark.readStream.format("kafka")`
   - Options: bootstrap.servers, subscribe, startingOffsets
   - Parse JSON with `from_json(col("value").cast("string"), schema)`
   - Extract payload fields
   - Full code example from RESEARCH.md Pattern 5

6. **Watermarks и агрегации** (~200 words)
   - `withWatermark("event_time", "10 minutes")`
   - Why watermarks matter: late data handling
   - Window aggregations with `window()` function
   - Warning about Pitfall 4 (missing watermarks = silent data loss)
   - Code example with groupBy window aggregation

7. **Checkpoint и восстановление** (~150 words)
   - Why checkpoints: exactly-once, fault recovery
   - `checkpointLocation` configuration
   - Warning about Pitfall 3 (shared dirs, insufficient storage)
   - Never change shuffle partitions after checkpointing

8. **Output Modes: append, update, complete** (~100 words)
   - append: new rows only (default for append-only streams)
   - update: changed rows (for aggregations)
   - complete: full result table (small results only)

9. **Лабораторная работа: Streaming Aggregation** (~100 words)
   - Exercise: read CDC events, calculate 5-minute revenue per customer
   - Use watermarks and window aggregation
   - Write to console for testing

Include Mermaid diagram for Structured Streaming data flow.
Reference RESEARCH.md for code examples.
  </action>
  <verify>
- File exists at src/content/course/05-module-5/05-pyspark-structured-streaming.mdx
- Frontmatter has all required fields
- Contains SparkSession configuration
- Contains StructType schema definition for CDC
- Contains readStream.format("kafka") example
- Contains from_json parsing
- Contains withWatermark configuration
- Contains checkpoint configuration
- Contains output modes explanation
- Mermaid diagram present
- Russian explanatory text, English code
  </verify>
  <done>Lesson 05-pyspark-structured-streaming.mdx exists with Kafka integration, schema definition, watermarks, and lab exercise</done>
</task>

<task type="auto">
  <name>Task 2: Create ETL/ELT Patterns Lesson</name>
  <files>src/content/course/05-module-5/06-etl-elt-patterns.mdx</files>
  <action>
Create lesson covering MOD5-06 (ETL/ELT patterns with CDC data).

**Frontmatter:**
- title: "ETL/ELT паттерны с CDC данными"
- description: "Проектирование data pipeline от CDC источника до data lake и warehouse"
- order: 6
- difficulty: "advanced"
- estimatedTime: 40
- topics: ["ETL", "ELT", "Data Lake", "Parquet", "Delta Lake", "CDC Patterns"]
- prerequisites: ["module-5/05-pyspark-structured-streaming"]

**Content structure (Russian explanatory text, English code):**

1. **ETL vs ELT: современный подход** (~150 words)
   - Traditional ETL: transform before load
   - Modern ELT: load raw, transform in warehouse
   - CDC enables both patterns
   - Mermaid diagram showing ETL vs ELT flows

2. **CDC как источник для Data Lake** (~150 words)
   - Advantages: incremental updates, change history, audit trail
   - Challenges: operation types, ordering, schema evolution
   - Target formats: Parquet, Delta Lake, Iceberg

3. **Паттерн: Append-Only History** (~200 words)
   - Keep all CDC events as append-only log
   - Add metadata: _operation, _cdc_timestamp, _processed_at
   - Partition by processed date for efficient queries
   - Code example from RESEARCH.md Pattern 6

4. **Паттерн: Separate Streams by Operation** (~200 words)
   - Filter CDC events by op type (c, u, d)
   - Route inserts to append table
   - Route updates to upsert logic
   - Route deletes to soft-delete or remove
   - Code example showing filter and separate writes

5. **Паттерн: Merge/Upsert с Delta Lake** (~200 words)
   - Delta Lake MERGE for atomic upsert
   - Match on primary key, update if exists, insert if not
   - Handle deletes with WHEN MATCHED AND source.op = 'd' DELETE
   - Code example (conceptual - Delta Lake specific syntax)

6. **Streaming to Parquet** (~150 words)
   - `writeStream.format("parquet")`
   - Partitioning strategy for CDC: by date, by table
   - Trigger options: processingTime, once, availableNow
   - Code example from RESEARCH.md Pattern 6

7. **Exactly-Once в PySpark Streaming** (~100 words)
   - Reference RESEARCH.md Open Question 3
   - PySpark provides at-least-once for Kafka sinks
   - For exactly-once: idempotent downstream or use PyFlink
   - Checkpoints provide fault tolerance, not exactly-once

8. **Мониторинг streaming pipeline** (~100 words)
   - StreamingQueryListener for metrics
   - Track: inputRowsPerSecond, processedRowsPerSecond, batchDuration
   - Alert on processing lag

9. **Лабораторная работа: CDC to Data Lake** (~100 words)
   - Exercise: stream CDC events to Parquet with partitioning
   - Add operation metadata columns
   - Query resulting Parquet files with Spark SQL

Include Mermaid diagrams for ETL vs ELT and pipeline architecture.
Reference RESEARCH.md for code patterns.
  </action>
  <verify>
- File exists at src/content/course/05-module-5/06-etl-elt-patterns.mdx
- Frontmatter has all required fields
- Contains ETL vs ELT comparison
- Contains append-only history pattern
- Contains operation filtering (c, u, d separation)
- Contains writeStream to Parquet example
- Contains partitioning strategy
- Contains metadata columns (_operation, _cdc_timestamp)
- Contains exactly-once limitations note
- Mermaid diagrams present
- Russian explanatory text, English code
  </verify>
  <done>Lesson 06-etl-elt-patterns.mdx exists with ETL/ELT patterns, Parquet streaming, and lab exercise</done>
</task>

</tasks>

<verification>
- [ ] Both lesson files exist in src/content/course/05-module-5/
- [ ] Lesson order field is sequential (5, 6)
- [ ] Prerequisites chain correctly
- [ ] PySpark 4.1.1 compatible syntax
- [ ] StructType schema matches Debezium envelope
- [ ] Watermark and checkpoint patterns correct
- [ ] ETL/ELT patterns are practical and actionable
- [ ] Mermaid diagrams render properly
- [ ] Russian language for explanatory text
- [ ] English for code, config, technical terms
</verification>

<success_criteria>
1. Student can configure PySpark to read CDC events from Kafka
2. Student can define proper schema for Debezium envelope
3. Student can implement watermark-based windowed aggregations
4. Student can configure checkpoints for fault tolerance
5. Student can design CDC-to-data-lake pipeline with proper partitioning
6. Student understands exactly-once limitations in PySpark
</success_criteria>

<output>
After completion, create `.planning/phases/09-module-5-data-engineering/09-03-SUMMARY.md`
</output>
